{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5ce4683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/muratkahraman/Downloads/data-engineering-practices/Exercises/Exercise-6\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "062a5ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file Divvy_Trips_2020_Q1.csv as it does not contain the selected columns.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "field gender: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/bj_drwyj2tv6qnh9h_kqvht40000gn/T/ipykernel_73252/3450094556.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Load and process the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/73/bj_drwyj2tv6qnh9h_kqvht40000gn/T/ipykernel_73252/3450094556.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(spark, folder_path, selected_columns)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                 \u001b[0;31m# Convert the pandas DataFrame to a PySpark DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                                 \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark3/spark-3.3.1-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_pandas\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n",
      "\u001b[0;32m~/Downloads/spark3/spark-3.3.1-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mconverted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     def _convert_from_pandas(\n",
      "\u001b[0;32m~/Downloads/spark3/spark-3.3.1-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark3/spark-3.3.1-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mtupled_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark3/spark-3.3.1-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0minfer_dict_as_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferDictAsStruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         schema = reduce(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_dict_as_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer_timestamp_ntz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark3/spark-3.3.1-bin-hadoop3/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m         fields = [\n\u001b[0m\u001b[1;32m   1384\u001b[0m             StructField(\n\u001b[1;32m   1385\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_merge_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNullType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark3/spark-3.3.1-bin-hadoop3/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1383\u001b[0m         fields = [\n\u001b[1;32m   1384\u001b[0m             StructField(\n\u001b[0;32m-> 1385\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_merge_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNullType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark3/spark-3.3.1-bin-hadoop3/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: field gender: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def load_data(spark, folder_path, selected_columns):\n",
    "    zip_files = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".zip\")]\n",
    "    dfs = []\n",
    "\n",
    "    for zip_file in zip_files:\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            for file_info in zip_ref.infolist():\n",
    "                if file_info.filename.endswith('.csv') and not file_info.filename.startswith('__MACOSX/'):\n",
    "                    with zip_ref.open(file_info) as csv_file:\n",
    "                        try:\n",
    "                            # Read the CSV file into a pandas DataFrame using 'ISO-8859-1' encoding\n",
    "                            df = pd.read_csv(csv_file, encoding='ISO-8859-1')\n",
    "\n",
    "                            # Check if the selected columns exist in the DataFrame\n",
    "                            if all(col in df.columns for col in selected_columns):\n",
    "                                # Select only the desired columns\n",
    "                                df = df[selected_columns]\n",
    "\n",
    "                                # Convert the pandas DataFrame to a PySpark DataFrame\n",
    "                                df = spark.createDataFrame(df)\n",
    "                                dfs.append(df)\n",
    "                            else:\n",
    "                                print(f\"Skipping file {file_info.filename} as it does not contain the selected columns.\")\n",
    "                        except UnicodeDecodeError:\n",
    "                            print(f\"Error reading file {file_info.filename}. Skipping this file.\")\n",
    "\n",
    "    if dfs:\n",
    "        # Combine all DataFrames into a single DataFrame\n",
    "        combined_df = dfs[0] if len(dfs) == 1 else dfs[0].unionAll(dfs[1:])\n",
    "        return combined_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"AverageTripDuration\").getOrCreate()\n",
    "\n",
    "# Define the selected columns\n",
    "selected_columns = [\"trip_id\", \"start_time\", \"end_time\", \"bikeid\", \"tripduration\", \"from_station_id\", \"from_station_name\", \"to_station_id\", \"to_station_name\", \"usertype\", \"gender\", \"birthyear\"]\n",
    "\n",
    "# Specify the folder path containing the zip files\n",
    "folder_path = \"data\"  # Replace with your folder path\n",
    "\n",
    "# Load and process the data\n",
    "result = load_data(spark, folder_path, selected_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc03924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, avg\n",
    "\n",
    "def calculate_avg_trip_duration_per_day(data):\n",
    "    # Convert 'start_time' to a date type\n",
    "    data = data.withColumn('start_date', to_date(data['start_time']))\n",
    "\n",
    "    # Group by date and calculate the average trip duration\n",
    "    avg_duration_per_day = data.groupBy('start_date').agg(avg('tripduration').alias('avg_trip_duration'))\n",
    "\n",
    "    return avg_duration_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defc01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trips_per_day(data):\n",
    "    trips_per_day = data.groupBy('start_date').count()\n",
    "    return trips_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a521a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_trip_duration_by_gender(data):\n",
    "    # Group by 'gender' and calculate the average trip duration\n",
    "    avg_duration_by_gender = data.groupBy('gender').agg(avg('tripduration').alias('avg_trip_duration'))\n",
    "\n",
    "    return avg_duration_by_gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7083cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns=[\"trip_id\",\"start_time\",\"end_time\",\"bikeid\",\"tripduration\",\"from_station_id\",\"from_station_name\",\"to_station_id\",\"to_station_name\",\"usertype\",\"gender\",\"birthyear\"]\n",
    "load_data(spark, \"data\",selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce716d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4b57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d4e19a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ride_id',\n",
       " 'rideable_type',\n",
       " 'started_at',\n",
       " 'ended_at',\n",
       " 'start_station_name',\n",
       " 'start_station_id',\n",
       " 'end_station_name',\n",
       " 'end_station_id',\n",
       " 'start_lat',\n",
       " 'start_lng',\n",
       " 'end_lat',\n",
       " 'end_lng',\n",
       " 'member_casual']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_file_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2f3b5200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_id',\n",
       " 'start_time',\n",
       " 'end_time',\n",
       " 'bikeid',\n",
       " 'tripduration',\n",
       " 'from_station_id',\n",
       " 'from_station_name',\n",
       " 'to_station_id',\n",
       " 'to_station_name',\n",
       " 'usertype',\n",
       " 'gender',\n",
       " 'birthyear']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_file_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4f537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Divvy_Trips_2020_Q1.csv, Record Count: 426887\n",
      "File: __MACOSX/._Divvy_Trips_2020_Q1.csv, Record Count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Divvy_Trips_2019_Q4.csv, Record Count: 704054\n",
      "File: __MACOSX/._Divvy_Trips_2019_Q4.csv, Record Count: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "\n",
    "def count_records_in_csv_files(folder_path, spark):\n",
    "    zip_files = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".zip\")]\n",
    "\n",
    "    for zip_file in zip_files:\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            for file_info in zip_ref.infolist():\n",
    "                if file_info.filename.endswith('.csv'):\n",
    "                    with zip_ref.open(file_info) as csv_file:\n",
    "                        # Create a temporary file to store the CSV contents\n",
    "                        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                            temp_file.write(csv_file.read())\n",
    "                            temp_file_name = temp_file.name\n",
    "\n",
    "                        # Read the CSV file from the temporary file and create a DataFrame\n",
    "                        try:\n",
    "                            df = spark.read.option(\"header\", \"true\").csv(temp_file_name, inferSchema=True)\n",
    "                            # Print the file name and the count of records in the DataFrame\n",
    "                            print(f\"File: {file_info.filename}, Record Count: {df.count()}\")\n",
    "                        except:\n",
    "                            # Handle any errors that occur during reading\n",
    "                            print(f\"Skipping file {file_info.filename} due to an error\")\n",
    "\n",
    "                        # Remove the temporary file\n",
    "                        os.remove(temp_file_name)\n",
    "\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CountCSVRecords\").getOrCreate()\n",
    "\n",
    "# Define the path to the folder containing the zip files\n",
    "folder_path = \"data\"  # Replace with the path to your data folder\n",
    "\n",
    "# Call the function to count records in CSV files\n",
    "count_records_in_csv_files(folder_path, spark)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c091886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_records_in_csv_files(folder_path, spark):\n",
    "    zip_files = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".zip\")]\n",
    "\n",
    "    for zip_file in zip_files:\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            for file_info in zip_ref.infolist():\n",
    "                if file_info.filename.endswith('.csv'):\n",
    "                    with zip_ref.open(file_info) as csv_file:\n",
    "                        # Create a temporary file to store the CSV contents\n",
    "                        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                            temp_file.write(csv_file.read())\n",
    "                            temp_file_name = temp_file.name\n",
    "\n",
    "                        # Read the CSV file from the temporary file and create a DataFrame\n",
    "                        try:\n",
    "                            df = spark.read.option(\"header\", \"true\").csv(temp_file_name, inferSchema=True)\n",
    "                            record_count = df.count()\n",
    "                            if record_count > 0:\n",
    "                                # Print the file name and the count of records in the DataFrame\n",
    "                                print(f\"File: {file_info.filename}, Record Count: {record_count}\")\n",
    "                        except:\n",
    "                            # Handle any errors that occur during reading\n",
    "                            print(f\"Skipping file {file_info.filename} due to an error\")\n",
    "\n",
    "                        # Remove the temporary file\n",
    "                        os.remove(temp_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ffd25b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/26 22:42:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Divvy_Trips_2020_Q1.csv, Record Count: 426887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Divvy_Trips_2019_Q4.csv, Record Count: 704054\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CountCSVRecords\").getOrCreate()\n",
    "\n",
    "# Define the path to the folder containing the zip files\n",
    "folder_path = \"data\"  # Replace with the path to your data folder\n",
    "\n",
    "# Call the function to count records in CSV files\n",
    "count_records_in_csv_files(folder_path, spark)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b705451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "def trips_per_day(df):\n",
    "    # Extract the date from the 'start_time' column\n",
    "    df = df.withColumn(\"start_date\", date_format(\"start_time\", \"yyyy-MM-dd\"))\n",
    "    # Group by date and count the number of trips\n",
    "    result = df.groupBy(\"start_date\").count()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b643f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popular_starting_stations_by_month(df):\n",
    "    # Extract the month from the 'start_time' column\n",
    "    df = df.withColumn(\"start_month\", date_format(\"start_time\", \"yyyy-MM\"))\n",
    "    # Group by month and starting station, and count the trips\n",
    "    result = df.groupBy(\"start_month\", \"from_station_name\").count()\n",
    "    # Find the station with the highest count in each month\n",
    "    result = result.orderBy(\"start_month\", col(\"count\").desc()).groupBy(\"start_month\").agg(\n",
    "        first(\"from_station_name\").alias(\"most_popular_starting_station\")\n",
    "    )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e0b02d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,dense_rank\n",
    "\n",
    "def top_trip_stations_last_two_weeks(df):\n",
    "    # Calculate the date difference from the current date\n",
    "    df = df.withColumn(\"days_ago\", datediff(current_date(), \"start_time\"))\n",
    "    # Filter the data for the last two weeks\n",
    "    df = df.filter((col(\"days_ago\") >= 0) & (col(\"days_ago\") <= 14))\n",
    "    # Create a window specification for ranking\n",
    "    window_spec = Window.partitionBy(\"start_date\").orderBy(col(\"count\").desc())\n",
    "    # Rank the stations by the number of trips for each day\n",
    "    result = df.withColumn(\"station_rank\", rank().over(window_spec))\n",
    "    # Filter the top 3 stations for each day\n",
    "    result = result.filter(col(\"station_rank\") <= 3).drop(\"station_rank\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0d7ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "def average_trip_duration_by_gender(df):\n",
    "    result = df.groupBy(\"gender\").agg(avg(\"tripduration\").alias(\"avg_trip_duration\"))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4789eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_ages_longest_and_shortest_trips(df):\n",
    "    result = df.groupBy(\"birthyear\").agg(avg(\"tripduration\").alias(\"avg_trip_duration\"))\n",
    "    # Sort by average trip duration and select the top 10 and bottom 10 ages\n",
    "    result = result.orderBy(\"avg_trip_duration\", ascending=False).limit(10)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a8adcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_analysis(df):\n",
    "    # Call each function and save the results to CSV files\n",
    "    trips_per_day_result = trips_per_day(df)\n",
    "    trips_per_day_result.write.csv(\"reports/trips_per_day.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "    popular_stations_result = popular_starting_stations_by_month(df)\n",
    "    popular_stations_result.write.csv(\"reports/popular_starting_stations_by_month.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "    top_stations_result = top_trip_stations_last_two_weeks(df)\n",
    "    top_stations_result.write.csv(\"reports/top_trip_stations_last_two_weeks.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "    gender_duration_result = average_trip_duration_by_gender(df)\n",
    "    gender_duration_result.write.csv(\"reports/average_trip_duration_by_gender.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "    age_duration_result = top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf523347",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fe58195f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file Divvy_Trips_2020_Q1.csv as it does not contain the selected columns.\n",
      "Skipping file __MACOSX/._Divvy_Trips_2020_Q1.csv as it does not contain the selected columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file __MACOSX/._Divvy_Trips_2019_Q4.csv as it does not contain the selected columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import date_format, datediff, col, rank, avg, first,desc,row_number,lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DivvyTripsAnalysis\").getOrCreate()\n",
    "\n",
    "# Define the function to load data from zipped CSV files\n",
    "def load_data(folder_path):\n",
    "    zip_files = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".zip\")]\n",
    "    data_frames = []\n",
    "\n",
    "    for zip_file in zip_files:\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            for file_info in zip_ref.infolist():\n",
    "                if file_info.filename.endswith('.csv'):\n",
    "                    with zip_ref.open(file_info) as csv_file:\n",
    "                        try:\n",
    "                            # Create a temporary file to store the CSV contents\n",
    "                            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                                temp_file.write(csv_file.read())\n",
    "                                temp_file_name = temp_file.name\n",
    "\n",
    "                            # Read the CSV file from the temporary file and create a DataFrame\n",
    "                            df = spark.read.option(\"header\", \"true\").csv(temp_file_name, inferSchema=True)\n",
    "                            if all(col_name in df.columns for col_name in selected_columns):\n",
    "                                data_frames.append(df)\n",
    "                            else:\n",
    "                                print(f\"Skipping file {file_info.filename} as it does not contain the selected columns.\")\n",
    "                        except Exception as e:\n",
    "                            # Handle any errors that occur during reading\n",
    "                            print(f\"Skipping file {file_info.filename} due to an error: {str(e)}\")\n",
    "\n",
    "                            # Remove the temporary file in case of an error\n",
    "                            os.remove(temp_file_name)\n",
    "    \n",
    "    return data_frames\n",
    "\n",
    "# Define the function to combine DataFrames into one DataFrame\n",
    "def combine_dataframes(data_frames):\n",
    "    if data_frames:\n",
    "        combined_df = data_frames[0]\n",
    "        for i in range(1, len(data_frames)):\n",
    "            combined_df = combined_df.union(data_frames[i])\n",
    "        return combined_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define the selected columns to filter by\n",
    "selected_columns = [\"trip_id\", \"start_time\", \"end_time\", \"bikeid\", \"tripduration\", \"from_station_id\", \"from_station_name\",\n",
    "                    \"to_station_id\", \"to_station_name\", \"usertype\", \"gender\", \"birthyear\"]\n",
    "\n",
    "# Define the path to the folder containing the zip files\n",
    "folder_path = \"data\"  # Replace with the path to your data folder\n",
    "\n",
    "# Load data and combine DataFrames\n",
    "data_frames = load_data(folder_path)\n",
    "combined_df = combine_dataframes(data_frames)\n",
    "\n",
    "# Check if combined_df is not None\n",
    "if combined_df:\n",
    "    \n",
    "    \n",
    "    # 1. What is the average trip duration per day?\n",
    "    def average_trip_duration_per_day(df: DataFrame):\n",
    "        result = df.withColumn(\"start_date\", date_format(\"start_time\", \"yyyy-MM-dd\"))\n",
    "        result = result.dropDuplicates()\n",
    "        result = result.groupBy(\"start_date\").agg(avg(\"tripduration\").alias(\"avg_trip_duration\"))\n",
    "        result = result.orderBy(\"start_date\")  # Sort by start_date in ascending order\n",
    "        result.write.csv(\"reports/average_trip_duration_per_day.csv\", header=True, mode=\"overwrite\")\n",
    "    \n",
    "    # 2. How many trips were taken each day?\n",
    "    def trips_per_day(df: DataFrame):\n",
    "        result = df.withColumn(\"start_date\", date_format(\"start_time\", \"yyyy-MM-dd\"))\n",
    "        result = result.dropDuplicates()\n",
    "        result = result.groupBy(\"start_date\").count()\n",
    "        result = result.orderBy(\"start_date\") \n",
    "        result.write.csv(\"reports/trips_per_day.csv\", header=True, mode=\"overwrite\")\n",
    "    \n",
    "    # 3. What was the most popular starting trip station for each month?\n",
    "    def popular_starting_stations_by_month(df: DataFrame):\n",
    "        result = df.withColumn(\"start_month\", date_format(\"start_time\", \"yyyy-MM\"))\n",
    "        result = result.dropDuplicates()\n",
    "        result = result.groupBy(\"start_month\", \"from_station_id\", \"from_station_name\").count()\n",
    "        result = result.orderBy(\"count\").sort(desc(\"count\"))\n",
    "        window = Window.partitionBy(\"start_month\").orderBy(desc(\"count\"))\n",
    "        result=result.withColumn(\"station_rank\", row_number().over(window))\n",
    "        result = result.filter(col(\"station_rank\") == 1).drop(\"station_rank\")\n",
    "        result.write.csv(\"reports/popular_starting_stations_by_month.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "    # 4. What were the top 3 trip stations each day for the last two weeks?\n",
    "    def top_trip_stations_last_two_weeks(df: DataFrame):\n",
    "        result = df.withColumn(\"start_date\", date_format(\"start_time\", \"yyyy-MM-dd\"))\n",
    "        result = result.dropDuplicates()\n",
    "        last_date = result.agg({\"start_date\": \"max\"}).collect()[0][0]\n",
    "        result = result.withColumn(\"days_ago\", datediff(lit(last_date), \"start_date\"))\n",
    "        result = result.filter((col(\"days_ago\") >= 0) & (col(\"days_ago\") <= 14))\n",
    "        result = result.groupBy(\"start_date\", \"from_station_id\", \"from_station_name\").count()\n",
    "        result = result.orderBy(\"count\").sort(desc(\"count\"))\n",
    "        window = Window.partitionBy(\"start_date\").orderBy(desc(\"count\"))\n",
    "        result=result.withColumn(\"station_rank\", row_number().over(window))\n",
    "        result = result.filter(col(\"station_rank\") <= 3).drop(\"station_rank\")\n",
    "        result.write.csv(\"reports/top_trip_stations_last_two_weeks.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "        \n",
    "    # 5. Do Males or Females take longer trips on average?\n",
    "    def average_trip_duration_by_gender(df: DataFrame):\n",
    "        result = df.withColumn(\"tripduration\", col(\"tripduration\").cast(DoubleType()))\n",
    "        result = df.groupBy(\"gender\").agg(avg(\"tripduration\").alias(\"avg_trip_duration\"))\n",
    "        result.write.csv(\"reports/average_trip_duration_by_gender.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "    # 6. What is the top 10 ages of those that take the longest trips, and shortest?\n",
    "    def top_10_ages_longest_and_shortest_trips(df: DataFrame):\n",
    "        result = df.withColumn(\"birthyear\", col(\"birthyear\").cast(DoubleType()))\n",
    "        result = df.groupBy(\"birthyear\").agg(avg(\"tripduration\").alias(\"avg_trip_duration\"))\n",
    "        result_longest = result.orderBy(col(\"avg_trip_duration\").desc()).limit(10)\n",
    "        result_shortest = result.orderBy(col(\"avg_trip_duration\")).limit(10)\n",
    "        result_longest.write.csv(\"reports/top_10_ages_longest_trips.csv\", header=True, mode=\"overwrite\")\n",
    "        result_shortest.write.csv(\"reports/top_10_ages_shortest_trips.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "    trips_per_day(combined_df)\n",
    "    average_trip_duration_per_day(combined_df)\n",
    "    popular_starting_stations_by_month(combined_df)\n",
    "    top_trip_stations_last_two_weeks(combined_df)\n",
    "    average_trip_duration_by_gender(combined_df)\n",
    "    top_10_ages_longest_and_shortest_trips(combined_df)\n",
    "\n",
    "else:\n",
    "    print(\"No valid data loaded.\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca609c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Exercise6\").enableHiveSupport().getOrCreate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
